{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report - Project Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Mechanism\n",
    "\n",
    "In an episode, agent choose an action, and enviroment changes accordingly. \n",
    "Inside an eposide, agent take actions according to its preference. envorioment feed back a reward accordingly. then agent update the knowledge accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dqn(agent,...):\n",
    "    ...\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        ...\n",
    "        for t in range(max_t): # max interations in an episode\n",
    "            action = agent.act(state) # agent choose an action according to the agent's preference\n",
    "            env_info = env.step(action)[brain_name] # environment changes via action\n",
    "            ...\n",
    "            agent.step(state, action, reward, next_state, done) # agent update knowledge according to reward.\n",
    "            state = next_state # update state\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Training\n",
    "\n",
    "#### Agent Definition\n",
    "agent is defined with parameters of state space size and actions space size, with other hypermeters.\n",
    "```python\n",
    "agent = Agent(state_size=37, action_size=4,seed =0, hidden = hidden)\n",
    "```\n",
    "agent has 2 main funcitons:\n",
    "\n",
    "1. choose action in current state according to its knowledge with \"Agent.act(self,state)\"\n",
    "```python\n",
    "action_values = self.qnetwork_local(state) # rank all action perferences of current state. \n",
    "return np.argmax(action_values.cpu().data.numpy()) # choose the best action accordingly.\n",
    "```  \n",
    "2. update knowledge of choosed action in last state according to reward form environment with \"Agent.learn(experience)\" regularly in time and randomly in replaying experience.\n",
    "```python\n",
    "states, actions, rewards, next_states, dones = experiences # get the samples to be learned\n",
    "```\n",
    "loss is measured according to \"predicted q value\" and \"learned q value\" from seperated network, prediction is based on the previous knowledge, learning is based on currently new learned knowledge, then update the pervious knowledge accordingly.\n",
    "```python\n",
    "Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "Q_targets = rewards + (gamma * Q_targets_next * (1 - dones)) # new knowledge in current state with reward updated\n",
    "Q_expected = self.qnetwork_local(states).gather(1, actions) # previous knowledge in current state\n",
    "loss = F.mse_loss(Q_expected, Q_targets)\n",
    "loss.backward() # update knowledge \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Inteligence\n",
    "\n",
    "Inteligence is modeled by deep neuron network. It is a fully connected layers with amount of neurons according to hidden-dims parameters, in this project 3 hidden layer setting is tested, result shown in Navigation.ipynb\n",
    "```\n",
    "QNetwork(\n",
    "  (fcs): ModuleList(\n",
    "    (0): Linear(in_features=37, out_features=64, bias=True)\n",
    "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
    "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
    "  )\n",
    "  (out): Linear(in_features=64, out_features=4, bias=True)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "source": [
    "### Hyperparameters\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Result plots\n",
    "\n",
    "![64_64_64](resources/64_64_64.png \"64 \\times 64 \\time 64\")\n",
    "![32_64_32](resources/32_64_32.png \"64 \\times 64 \\time 64\")\n",
    "![32_32_32](resources/32_32_32.png \"64 \\times 64 \\time 64\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Future works\n",
    "\n",
    "* To improve the performance, I an apply Double DQN, Dueling DQN, Prioritized Experienced Replay.\n",
    "* Train network from image data directly"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}